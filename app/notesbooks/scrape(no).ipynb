{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\nermi\\appdata\\local\\temp\\pip-req-build-5bevg0p_\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 614d4c2029a62d348ca56598f87c425966aaec66\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from snscrape==0.7.0.20230622) (2.32.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from snscrape==0.7.0.20230622) (5.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from snscrape==0.7.0.20230622) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from snscrape==0.7.0.20230622) (3.16.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from beautifulsoup4->snscrape==0.7.0.20230622) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (1.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\nermi\\AppData\\Local\\Temp\\pip-req-build-5bevg0p_'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\certifi\\cacert.pem\n",
      "SSL Connection Successful!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "import socket\n",
    "\n",
    "# Print the path to the certifi bundle\n",
    "print(certifi.where())\n",
    "\n",
    "# Check if the SSL context can create a connection\n",
    "try:\n",
    "    context = ssl.create_default_context()\n",
    "    with context.wrap_socket(socket.socket(), server_hostname='twitter.com') as s:\n",
    "        s.connect(('twitter.com', 443))\n",
    "    print(\"SSL Connection Successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"SSL Connection Failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <!DOCTYPE html>\n",
      "    <head>\n",
      "      <title>x.com</title>\n",
      "      <meta http-equiv=\"refresh\" content=\"0; url = https://twitter.com/x/migrate?tok=7b2265223a222f7365617263683f663d6c697665266c616e673d656e26713d2532344e464c582b73696e6365253341323032342d30392d32302b756e74696c253341323032342d31302d3031267372633d7370656c6c696e675f657870616e73696f6e5f7265766572745f636c69636b222c2274223a313732383437393935387dc620ea98b5ad92805b11f3c4ffa3ec17\" />\n",
      "      <meta charset=\"utf-8\">\n",
      "      <meta name=\"viewport\" content=\"width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover\">\n",
      "\n",
      "      <link rel=\"preconnect\" href=\"//abs.twimg.com\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//abs.twimg.com\">\n",
      "      <link rel=\"preconnect\" href=\"//api.twitter.com\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//api.twitter.com\">\n",
      "      <link rel=\"preconnect\" href=\"//api.x.com\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//api.x.com\">\n",
      "      <link rel=\"preconnect\" href=\"//pbs.twimg.com\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//pbs.twimg.com\">\n",
      "      <link rel=\"preconnect\" href=\"//t.co\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//t.co\">\n",
      "      <meta http-equiv=\"onion-location\" content=\"https://twitter3e4tixl4xyajtrzo62zg5vztmjuricljdp2c5kshju4avyoid.onion/\" />\n",
      "      <meta property=\"fb:app_id\" content=\"2231777543\" />\n",
      "      <meta content=\"X (formerly Twitter)\" property=\"og:site_name\" />\n",
      "      <meta name=\"google-site-verification\" content=\"600dQ0pZYsH2xOFt4hYmf5f5NpjCbWE_qk5Y04dErYM\" />\n",
      "      <meta name=\"facebook-domain-verification\" content=\"x6sdcc8b5ju3bh8nbm59eswogvg6t1\" />\n",
      "      <meta name=\"mobile-web-app-capable\" content=\"yes\" />\n",
      "      <meta name=\"apple-mobile-web-app-title\" content=\"Twitter\" />\n",
      "      <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"white\" />\n",
      "      <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/opensearch.xml\" title=\"Twitter\"/>\n",
      "      <link rel=\"apple-touch-icon\" sizes=\"192x192\" href=\"https://abs.twimg.com/responsive-web/client-web/icon-ios.77d25eba.png\" />\n",
      "      <meta name=\"twitter-site-verification\" content=\"AUVDWo1JpbjI22xjTe5JOvTAWuW9bK41CpxYxCeCjH97mEVp7rtiHcvdOaUksJrG\" />\n",
      "      <link rel=\"manifest\" href=\"/manifest.json\" crossorigin=\"use-credentials\" />\n",
      "      <link rel=\"mask-icon\" sizes=\"any\" href=\"https://abs.twimg.com/responsive-web/client-web/icon-svg.ea5ff4aa.svg\" color=\"#1D9BF0\" />\n",
      "      <link rel=\"shortcut icon\" href=\"https://abs.twimg.com/favicons/twitter-pip.3.ico\" />\n",
      "      <meta name=\"theme-color\" content=\"#000000\" />\n",
      "      <script type=\"text/javascript\" charset=\"utf-8\" nonce=\"YTQzZTE1YTQtOTEzMi00ZTk5LWI0ZTYtZWQ2YTQzYWU0ZDA5\">document.location = \"https://twitter.com/x/migrate?tok=7b2265223a222f7365617263683f663d6c697665266c616e673d656e26713d2532344e464c582b73696e6365253341323032342d30392d32302b756e74696c253341323032342d31302d3031267372633d7370656c6c696e675f657870616e73696f6e5f7265766572745f636c69636b222c2274223a313732383437393935387dc620ea98b5ad92805b11f3c4ffa3ec17\"</script>\n",
      "      </head>\n",
      "    <body style=\"background: #000\">\n",
      "\n",
      "    </body>\n",
      "    </html>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(\n",
    "        'https://twitter.com/search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click',\n",
    "        verify=r'C:\\Users\\nermi\\Desktop\\pi5eme\\app\\notesbooks\\cacert.pem'  # Adjust the path accordingly\n",
    "    )\n",
    "    print(response.text)  # Or use response.json() based on your needs\n",
    "except requests.exceptions.SSLError as ssl_error:\n",
    "    print(f\"SSL Error: {ssl_error}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tweet scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\"))\n",
      "4 requests to https://twitter.com/search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click failed, giving up.\n",
      "Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 4 requests to https://twitter.com/search?f=live&lang=en&q=%24NFLX+since%3A2024-09-20+until%3A2024-10-01&src=spelling_expansion_revert_click failed, giving up.\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Create a session with SSL verification\n",
    "session = requests.Session()\n",
    "session.verify = r'C:\\Users\\nermi\\Desktop\\pi5eme\\app\\notesbooks\\cacert.pem'  # Adjust the path accordingly\n",
    "\n",
    "# Define the query parameters\n",
    "text_query = \"$NFLX\"\n",
    "since_date = \"2024-09-20\"  # Broaden the start date\n",
    "until_date = \"2024-10-01\"\n",
    "\n",
    "# List to store tweets\n",
    "tweets_list = []\n",
    "\n",
    "# Scraping tweets\n",
    "try:\n",
    "    print(\"Starting tweet scraping...\")\n",
    "    for tweet in sntwitter.TwitterSearchScraper(f'{text_query} since:{since_date} until:{until_date}').get_items():\n",
    "        tweets_list.append([tweet.date, tweet.content])\n",
    "        if len(tweets_list) >= 100:  # Limit to 100 tweets\n",
    "            break\n",
    "    print(\"Scraping completed.\")\n",
    "\n",
    "    # Creating a DataFrame from the scraped tweets\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Date', 'Tweet'])\n",
    "\n",
    "    # Save to JSON file\n",
    "    tweets_df.to_json('text-query-tweets.json', orient='records', lines=True)\n",
    "\n",
    "    # Check if the DataFrame is empty\n",
    "    if tweets_df.empty:\n",
    "        print(\"No tweets found for the specified query and date range.\")\n",
    "    else:\n",
    "        print(\"Tweets successfully scraped:\")\n",
    "        print(tweets_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['date', 'renderedContent', 'lang'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m tweets_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-query-tweets.json\u001b[39m\u001b[38;5;124m'\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# tweets_df2 = pd.read_json('text-query-tweets2.json', lines=True)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Merging both dataframes as a single dataframe\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# tweets_df = pd.concat([tweets_df1,tweets_df2],ignore_index=True)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Selecting the important columns only wich are Data,renderContent and Lang\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tweets_content \u001b[38;5;241m=\u001b[39m \u001b[43mtweets_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrenderedContent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Choosing the tweets in english language only\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tweets_content \u001b[38;5;241m=\u001b[39m tweets_content[tweets_content[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nermi\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['date', 'renderedContent', 'lang'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Reading jason files as dataframes\n",
    "tweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "# tweets_df2 = pd.read_json('text-query-tweets2.json', lines=True)\n",
    "# Merging both dataframes as a single dataframe\n",
    "# tweets_df = pd.concat([tweets_df1,tweets_df2],ignore_index=True)\n",
    "# Selecting the important columns only wich are Data,renderContent and Lang\n",
    "tweets_content = tweets_df.loc[:,['date','renderedContent','lang']]\n",
    "# Choosing the tweets in english language only\n",
    "tweets_content = tweets_content[tweets_content['lang']=='en']\n",
    "# Dropping the lang column\n",
    "tweets_content.drop(\"lang\",axis=1,inplace=True)\n",
    "# Download the CSV file result on the current folder.\n",
    "tweets_content.to_csv('Ntweets.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
